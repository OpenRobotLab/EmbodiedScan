from collections import defaultdict

import torch
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer
from scipy.spatial.distance import cosine
from sentence_transformers import SentenceTransformer, util
from transformers import AutoModel, AutoTokenizer

from mmscan_tool.utils.lang_utils import (exact_match_score,
                                          special_token_filter)


def to_coco(kvs, keys):
    res = defaultdict(list)
    for k in keys:
        if k in kvs:
            caps = kvs[k]
            for c in caps:
                res[k].append({'caption': c})
        else:
            res[k].append({'caption': ''})
    return res


def coco_evaluate(batch_input):
    """Calculate the extract matching score for each item.
    Args:
        batch_input(list[dict]):
            [{
                "pred": [str],
                "gt":[str,...]
            },...]

    Returns:
        dict, dict: final_scores stores the score of each metric
    """

    prediction = {}
    ground_truths = {}

    for _input in batch_input:
        prediction[_input['ID']] = _input['pred']
        ground_truths[_input['ID']] = _input['gt']

    scorers = [
        (Bleu(4), ['Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4']),
        (Meteor(), 'METEOR'),
        (Rouge(), 'ROUGE_L'),
        (Cider(), 'CIDEr'),
        (Spice(), 'SPICE'),
    ]

    tokenizer = PTBTokenizer()
    ref_sent = ground_truths
    hypo_sent = prediction
    final_scores = {}
    final_list = {}
    ref_coco = tokenizer.tokenize(to_coco(ref_sent, ref_sent.keys()))
    hypo_coco = tokenizer.tokenize(to_coco(hypo_sent, ref_sent.keys()))
    for scorer, method in scorers:
        score, scores = scorer.compute_score(ref_coco, hypo_coco)
        if type(score) == list:
            for m, s, s_ in zip(method, score, scores):
                final_scores[m] = s
                final_list[m] = s_
        else:
            final_scores[method] = score
            final_list[method] = scores

    return final_scores, final_list


def em_evaluation(batch_input):
    """Calculate the extract matching score for each item.
    Args:
        batch_input(list[dict]):
            [{
                "pred": [str],
                "gt":[str,...]
            },...]

    Returns:
        list[float]: (refined) extract matching score for each item
    """
    # EM
    em_result = []
    for _input in batch_input:
        pred = _input['pred'][0]
        gts = _input['gt']
        if pred in gts:
            em_result.append(1)
        else:
            em_result.append(0)

    # refined EM
    refine_em_result = []
    cnt = []
    for _input in batch_input:
        correct = 0
        pred = _input['pred'][0]
        gts = _input['gt']
        cnt = []
        if len(pred.split()) == 0:
            pred = '@@@@@@@@-= Empty Answer =-@@@@@@@@@'
        for gt in gts:
            if pred == gt:
                correct = 1
                continue
            elif ''.join(pred.split()) in ''.join(gt.split()):
                correct = 1
                continue
            elif ''.join(gt.split()) in ''.join(pred.split()):
                correct = 1
                continue
        refine_em_result.append(correct)
    return em_result, refine_em_result


class simcse_evaluator:

    def __init__(self, model_path, eval_bs=500) -> None:
        self.eval_bs = eval_bs
        if len(model_path) == 0:
            model_path = 'princeton-nlp/sup-simcse-roberta-large'
        self.simcse_tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.simcse_model = AutoModel.from_pretrained(model_path).to('cuda')

    def __batch_evaluation__(self, all_pred, all_gt, gt_count):
        """Using Sentence Embeddings to calculate similarity between pred/gt in
        a batch.

        Args:
            gt_count(list[int]): stores number of possible answers to a question
            all_pred(list[str]): all prediction
            all_gt(list[str]): all ground truth
            tips: len(all_gt)>=len(all_pred) because there may be multiple
                  gt answers for a question.

        Return:
            list[float]: Simcse similarity of each pred/gts pair.
        """
        len_of_pred = len(all_pred)
        with torch.no_grad():
            inputs = self.simcse_tokenizer(
                all_pred + all_gt,
                padding=True,
                truncation=True,
                return_tensors='pt',
            ).to('cuda')
            simcse_embeddings = self.simcse_model(
                **inputs, output_hidden_states=True,
                return_dict=True).pooler_output

        all_pred_simcse_embed = simcse_embeddings[:len_of_pred]
        all_gt_simcse_embed = simcse_embeddings[len_of_pred:]
        all_simcse_sim = []

        accumulated = 0
        for i in range(len(all_pred)):
            simcse_similarity = -100
            for j in range(accumulated, accumulated + gt_count[i]):
                simcse_similarity = max(
                    simcse_similarity,
                    1 - cosine(
                        all_pred_simcse_embed[i].cpu().detach().numpy(),
                        all_gt_simcse_embed[j].cpu().detach().numpy(),
                    ),
                )

            all_simcse_sim.append(simcse_similarity)
            accumulated += gt_count[i]
        torch.cuda.empty_cache()
        return all_simcse_sim

    def evaluation(self, batch_input):
        """Calculate the simcse similarity score for each item.
        Args:
            batch_input(list[dict]):
                [{
                    "pred": [str],
                    "gt":[str,...]
                },...]

        Returns:
            list[float]: simcse similarity for each item
        """
        all_simcse_similarity = []
        batch_lan_pred = []
        batch_lan_gt = []
        count_gt = []

        for idx, _item in enumerate(batch_input):
            batch_lan_pred.extend(_item['pred'])
            batch_lan_gt.extend(_item['gt'])
            count_gt.extend([len(_item['gt'])])

            if (idx + 1) % self.eval_bs == 0 or idx == len(batch_input) - 1:
                all_simcse_similarity += self.__batch_evaluation__(
                    batch_lan_pred, batch_lan_gt, count_gt)
                batch_lan_pred = []
                batch_lan_gt = []
                count_gt = []

        return all_simcse_similarity


class sbert_evaluator:

    def __init__(self, model_path, eval_bs=500) -> None:
        self.eval_bs = eval_bs
        if len(model_path) == 0:
            model_path = 'all-mpnet-base-v2'
        self.sbert_model = SentenceTransformer(model_path, device='cuda')

    def __batch_evaluation__(self, all_pred, all_gt, gt_count):
        """Using Sentence-BERT to calculate similarity between pred/gt in a
        batch.

        Args:
            gt_count(list[int]): stores number of possible answers to a question
            all_pred(list[str]): all prediction
            all_gt(list[str]): all ground truth
            tips: len(all_gt)>=len(all_pred) because there may be multiple
                  gt answers for a question.

        Return:
            list[float]: Sentence-BERT similarity of each pred/gts pair.
        """
        len_of_pred = len(all_pred)
        with torch.no_grad():
            sbert_embeddings = self.sbert_model.encode(all_pred + all_gt,
                                                       show_progress_bar=False,
                                                       device='cuda')

        all_pred_sbert_embed = sbert_embeddings[:len_of_pred]
        all_gt_sbert_embed = sbert_embeddings[len_of_pred:]
        all_sbert_sim = []

        accumulated = 0
        for i in range(len(all_pred)):
            sbert_similarity = -100
            for j in range(accumulated, accumulated + gt_count[i]):
                sbert_similarity = max(
                    sbert_similarity,
                    util.cos_sim(all_pred_sbert_embed[i],
                                 all_gt_sbert_embed[j])[0][0].item(),
                )
            all_sbert_sim.append(sbert_similarity)
            accumulated += gt_count[i]
        torch.cuda.empty_cache()
        return all_sbert_sim

    def evaluation(self, batch_input):
        """Calculate the simcse similarity score for each item.
        Args:
            batch_input(list[dict]):
                [{
                    "pred": [str],
                    "gt":[str,...]
                },...]

        Returns:
            list[float]: simcse similarity for each item
        """
        all_sbert_similarity = []
        batch_lan_pred = []
        batch_lan_gt = []
        count_gt = []

        for idx, _item in enumerate(batch_input):
            batch_lan_pred.extend(_item['pred'])
            batch_lan_gt.extend(_item['gt'])
            count_gt.extend([len(_item['gt'])])

            if (idx + 1) % self.eval_bs == 0 or idx == len(batch_input) - 1:
                all_sbert_similarity += self.__batch_evaluation__(
                    batch_lan_pred, batch_lan_gt, count_gt)
                batch_lan_pred = []
                batch_lan_gt = []
                count_gt = []

        return all_sbert_similarity
